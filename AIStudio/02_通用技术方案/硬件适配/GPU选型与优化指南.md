# GPU 硬件选型与优化指南

## 1. 主流 GPU 型号与对比

### 1.1. NVIDIA
*   **数据中心级**:
    *   **H100/H200**: （当前旗舰，适用于大规模训练和推理，Tensor Core 性能卓越。）
    *   **A100/A800**: （上一代旗舰，性价比高，广泛应用于各类 AI 任务。）
    *   **L40S/L4**: （推理优化型，适用于 CV、NLP 等多种推理场景。）
*   **消费级**:
    *   **RTX 4090**: （性能强劲，适用于个人开发者、小型团队进行模型研究和 Fine-tuning。）
    *   **RTX 3090**: （上一代卡皇，显存大，性价比选择。）

### 1.2. AMD
*   **MI300X/MI300A**: （对标 H100，在特定场景下有竞争力，生态正在快速发展。）

### 1.3. 国产芯片
*   （例如：华为昇腾、壁仞科技等，介绍其特点和适用场景。）

### 1.4. 选型矩阵
| 场景 | 推荐型号 | 显存建议 | 备注 |
| :--- | :--- | :--- | :--- |
| 大模型训练 (>=70B) | H100, A100 | >= 80GB | 建议多卡多节点 |
| 中等模型 Fine-tuning | A100, L40S, RTX 4090 | >= 24GB | |
| CV/NLP 推理 | L4, T4, A10 | >= 16GB | 关注推理延迟和吞吐 |
| 个人学习/研究 | RTX 4090, RTX 3090 | >= 24GB | |

## 2. 硬件适配与环境配置

### 2.1. 驱动安装
*   **NVIDIA Driver**: （介绍如何选择正确的驱动版本，以及安装步骤。）
*   **CUDA Toolkit**: （解释 CUDA 与驱动的关系，如何安装与项目匹配的 CUDA 版本。）
*   **cuDNN**: （加速深度学习计算的库，安装和配置方法。）

### 2.2. 容器化部署 (Docker & NVIDIA Container Toolkit)
*   （介绍使用 Docker 封装环境的好处，以及如何构建支持 GPU 的 Docker 镜像。）

## 3. 性能优化

### 3.1. 混合精度训练 (Mixed Precision)
*   **原理**: （解释 FP16/BF16 与 FP32 混合使用的原理，如何利用 Tensor Core 加速。）
*   **实践**: （在 PyTorch 中使用 `torch.cuda.amp` 的示例。）

### 3.2. 分布式训练
*   **数据并行 (Data Parallelism)**: （模型复制，数据切分。）
*   **模型并行 (Tensor/Pipeline Parallelism)**: （模型切分，适用于超大模型。）

### 3.3. 推理优化 (TensorRT)
*   **原理**: （介绍 TensorRT 如何进行图优化、内核融合、精度量化等。）
*   **流程**: （从模型（ONNX）到 TensorRT engine 的转换过程。）
