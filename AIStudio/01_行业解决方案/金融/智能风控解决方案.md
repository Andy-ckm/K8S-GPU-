# 金融解决方案

### 补完部分 1: 完整的 Python 代码

以下是您需要创建在 `backend/` 目录下的两个核心Python文件。

#### **`backend/scripts/ingest_data.py` (完整代码)**

这个脚本负责初始化所有数据源：将知识库文档向量化存入Milvus，并在PostgreSQL中创建表和模拟数据。

```python
import os
import time
from sqlalchemy import create_engine, text, inspect
from pymilvus import connections, utility, FieldSchema, CollectionSchema, DataType, Collection
from sentence_transformers import SentenceTransformer
from langchain.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- 配置 ---
# Milvus 配置
MILVUS_HOST = os.getenv("MILVUS_HOST", "localhost")
MILVUS_PORT = "19530"
COLLECTION_NAME = "financial_knowledge"
EMBEDDING_DIM = 1024  # bge-large-zh-v1.5 的维度

# PostgreSQL 配置
POSTGRES_USER = os.getenv("POSTGRES_USER", "fin_user")
POSTGRES_PASSWORD = os.getenv("POSTGRES_PASSWORD", "fin_password")
POSTGRES_DB = os.getenv("POSTGRES_DB", "fin_db")
POSTGRES_HOST = os.getenv("POSTGRES_HOST", "localhost")
DB_URL = f"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}/{POSTGRES_DB}"

# 数据与模型配置
KNOWLEDGE_BASE_PATH = "/app/knowledge_base"
EMBEDDING_MODEL = 'BAAI/bge-large-zh-v1.5'

def setup_milvus():
    """初始化Milvus，创建collection并导入知识库数据"""
    print("--- 开始初始化 Milvus ---")
    try:
        connections.connect("default", host=MILVUS_HOST, port=MILVUS_PORT)
        print("成功连接到 Milvus。")
    except Exception as e:
        print(f"连接 Milvus 失败: {e}")
        return

    # 1. 删除已存在的 collection，确保幂等性
    if utility.has_collection(COLLECTION_NAME):
        print(f"发现已存在的 Collection '{COLLECTION_NAME}'，正在删除...")
        utility.drop_collection(COLLECTION_NAME)
        time.sleep(1) # 等待删除完成

    # 2. 定义并创建新的 collection
    fields = [
        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
        FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
        FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM)
    ]
    schema = CollectionSchema(fields, "金融知识库")
    collection = Collection(COLLECTION_NAME, schema)
    print(f"Collection '{COLLECTION_NAME}' 创建成功。")

    # 3. 加载并分割文档
    print(f"从 '{KNOWLEDGE_BASE_PATH}' 加载文档...")
    loader = DirectoryLoader(KNOWLEDGE_BASE_PATH, glob="**/*.md", loader_cls=UnstructuredMarkdownLoader)
    documents = loader.load()
    if not documents:
        print("警告：在 knowledge_base 目录中未找到任何 .md 文件。")
        return

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    docs = text_splitter.split_documents(documents)
    texts = [doc.page_content for doc in docs]
    print(f"文档加载完毕，共分割成 {len(texts)} 个文本块。")

    # 4. 向量化并插入数据
    print(f"正在加载嵌入模型 '{EMBEDDING_MODEL}'...")
    model = SentenceTransformer(EMBEDDING_MODEL, device='cpu') # 明确使用CPU
    print("模型加载完毕，开始生成向量...")
    embeddings = model.encode(texts, show_progress_bar=True)

    print("开始向 Milvus 插入数据...")
    collection.insert([texts, embeddings.tolist()])
    collection.flush()
    print(f"数据插入完成，共 {collection.num_entities} 条。")

    # 5. 创建索引以加速搜索
    index_params = {
        "metric_type": "L2",
        "index_type": "IVF_FLAT",
        "params": {"nlist": 128}
    }
    collection.create_index("embedding", index_params)
    print("索引创建成功。")
    print("--- Milvus 初始化完成 ---")


def setup_postgres():
    """初始化PostgreSQL，创建表并插入模拟数据"""
    print("\n--- 开始初始化 PostgreSQL ---")
    try:
        engine = create_engine(DB_URL)
        with engine.connect() as connection:
            print("成功连接到 PostgreSQL。")
            inspector = inspect(engine)

            # 定义表结构
            behavior_log_table = "user_behavior_log"
            complaints_table = "user_complaints"

            # 删除旧表
            if inspector.has_table(complaints_table):
                print(f"发现已存在的表 '{complaints_table}'，正在删除...")
                connection.execute(text(f"DROP TABLE {complaints_table};"))
            if inspector.has_table(behavior_log_table):
                print(f"发现已存在的表 '{behavior_log_table}'，正在删除...")
                connection.execute(text(f"DROP TABLE {behavior_log_table};"))
            
            connection.commit()

            # 创建用户行为日志表
            create_behavior_log_sql = """
            CREATE TABLE user_behavior_log (
                id SERIAL PRIMARY KEY,
                user_id VARCHAR(50) NOT NULL,
                event_time TIMESTAMP NOT NULL,
                event_type VARCHAR(50),
                details TEXT
            );
            """
            connection.execute(text(create_behavior_log_sql))
            print(f"表 '{behavior_log_table}' 创建成功。")

            # 创建用户投诉表
            create_complaints_sql = """
            CREATE TABLE user_complaints (
                id SERIAL PRIMARY KEY,
                user_id VARCHAR(50),
                complaint_time TIMESTAMP NOT NULL,
                complaint_details TEXT,
                status VARCHAR(20) DEFAULT 'open'
            );
            """
            connection.execute(text(create_complaints_sql))
            print(f"表 '{complaints_table}' 创建成功。")

            # 插入模拟数据
            insert_log_sql = text("""
            INSERT INTO user_behavior_log (user_id, event_time, event_type, details)
            VALUES ('user_123', '2025-05-04 09:30:00', 'login', 'Login attempt failed using Face ID');
            """)
            connection.execute(insert_log_sql)
            print("已插入模拟用户行为日志。")
            
            connection.commit()

    except Exception as e:
        print(f"PostgreSQL 初始化失败: {e}")
        return
    print("--- PostgreSQL 初始化完成 ---")


if __name__ == "__main__":
    setup_milvus()
    setup_postgres()
    print("\n所有数据源初始化完成！")

```

#### **`backend/main.py` (完整代码)**

这是FastAPI应用的核心，包含了所有智能体的路由和业务逻辑。

```python
import os
import datetime
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import openai
from pymilvus import connections, Collection
from sentence_transformers import SentenceTransformer
from sqlalchemy import create_engine, text, insert

# --- 配置 ---
# 模型与服务连接
MILVUS_HOST = os.getenv("MILVUS_HOST", "localhost")
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
POSTGRES_USER = os.getenv("POSTGRES_USER", "fin_user")
POSTGRES_PASSWORD = os.getenv("POSTGRES_PASSWORD", "fin_password")
POSTGRES_DB = os.getenv("POSTGRES_DB", "fin_db")
POSTGRES_HOST = os.getenv("POSTGRES_HOST", "localhost")

# 模型名称
EMBEDDING_MODEL = 'BAAI/bge-large-zh-v1.5'
LLM_MODEL = 'qwen:72b'
COLLECTION_NAME = "financial_knowledge"

# --- 初始化 ---
app = FastAPI(title="金融多智能体AI解决方案")

# 1. 加载嵌入模型
print("正在加载嵌入模型...")
embed_model = SentenceTransformer(EMBEDDING_MODEL, device='cpu')
print("嵌入模型加载完毕。")

# 2. 连接 Milvus
connections.connect("default", host=MILVUS_HOST, port="19530")
milvus_collection = Collection(COLLECTION_NAME)
milvus_collection.load()
print("已连接并加载 Milvus Collection。")

# 3. 连接 PostgreSQL
DB_URL = f"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}/{POSTGRES_DB}"
engine = create_engine(DB_URL)
print("已创建 PostgreSQL 连接引擎。")

# 4. 配置 Ollama 客户端
ollama_client = openai.OpenAI(
    base_url=f"{OLLAMA_HOST}/v1",
    api_key='ollama',
)
print("Ollama 客户端配置完成。")

# --- Pydantic 模型 ---
class QueryRequest(BaseModel):
    query: str
    user_id: str = "user_123" # 示例用户ID

class ChatResponse(BaseModel):
    agent: str
    response: str

# --- Agent 逻辑 ---
def handle_marketing_query(query: str) -> str:
    """处理营销/知识库查询 (RAG)"""
    # 1. 向量化问题
    query_vector = embed_model.encode(query).tolist()

    # 2. 搜索相关文档
    search_params = {"metric_type": "L2", "params": {"nprobe": 10}}
    results = milvus_collection.search(
        data=[query_vector],
        anns_field="embedding",
        param=search_params,
        limit=3,
        output_fields=["text"]
    )
    context = "\n---\n".join([hit.entity.get('text') for hit in results[0]])

    # 3. 构建 Prompt 并调用 LLM
    prompt = f"""
    你是一个专业的金融营销专员。请基于以下背景知识，清晰、准确地回答用户的问题。
    如果背景知识无法回答，请礼貌地告知用户你暂时无法提供该信息。

    [背景知识]
    {context}

    [用户问题]
    {query}
    """
    response = ollama_client.chat.completions.create(
        model=LLM_MODEL,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

def handle_complaint_query(query: str, user_id: str) -> str:
    """处理投诉，与数据库交互"""
    context = "未查询到相关用户行为日志。"
    try:
        with engine.connect() as connection:
            # 1. 查询相关日志 (为简化，我们直接查询示例日志)
            log_query = text("SELECT details, event_time FROM user_behavior_log WHERE user_id = :user_id AND details LIKE '%failed%' ORDER BY event_time DESC LIMIT 1")
            result = connection.execute(log_query, {"user_id": user_id}).fetchone()
            if result:
                context = f"我们已经核实到您在{result.event_time.strftime('%Y年%m月%d日 %H:%M:%S')} 尝试{result.details}。"

            # 2. 记录新的投诉
            complaint_time = datetime.datetime.now()
            insert_stmt = text("INSERT INTO user_complaints (user_id, complaint_time, complaint_details) VALUES (:user_id, :time, :details)")
            connection.execute(insert_stmt, {"user_id": user_id, "time": complaint_time, "details": query})
            connection.commit()
            context += f" 您的反馈对我们至关重要，我们已将此次投诉于{complaint_time.strftime('%Y年%m月%d日 %H:%M:%S')}记录下来以便进一步分析和改进。"

    except Exception as e:
        print(f"数据库操作失败: {e}")
        context = "处理您的请求时系统出现内部错误。我们已记录此问题。"

    # 3. 构建 Prompt 并调用 LLM
    prompt = f"""
    你是一位经验丰富且富有同理心的客户投诉专员。你的任务是安抚用户情绪，并告知用户你已经采取的行动。

    [已知情况]
    {context}

    [用户抱怨]
    {query}

    请根据已知情况，生成一段专业、诚恳且有帮助的回复。首先要表示理解和歉意，然后说明你已经核实到的信息和记录的投诉，最后表达解决问题的意愿。
    """
    response = ollama_client.chat.completions.create(
        model=LLM_MODEL,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

# --- API 路由 ---
@app.post("/chat", response_model=ChatResponse)
def chat_router(request: QueryRequest):
    """分诊台 Agent: 根据用户问题路由到不同的专员"""
    query = request.query.lower()
    complaint_keywords = ["投诉", "失败", "不满", "登不上", "无法登录", "问题"]

    # 路由逻辑
    if any(keyword in query for keyword in complaint_keywords):
        agent_name = "投诉专员"
        response_text = handle_complaint_query(request.query, request.user_id)
    else:
        agent_name = "营销专员"
        response_text = handle_marketing_query(request.query)

    return ChatResponse(agent=agent_name, response=response_text)

@app.get("/")
def read_root():
    return {"status": "Fin-Agent-Suite is running."}

```

---

### 补完部分 2: `LICENSE` 文件

在您的项目根目录下，创建一个名为 `LICENSE` 的文件，并粘贴以下内容：

```
MIT License

Copyright (c) 2024 [Your Name or Organization]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```
**请记得将 `[Your Name or Organization]` 替换为您自己的GitHub用户名或组织名。**

---

### 补完部分 3: 优化后的 `README.md`

这是最终版本的 `README.md`，包含了更详细的管理命令和说明，可以直接使用。

# Fin-Agent-Suite: 本地化部署的金融多智能体AI解决方案

这是一个基于多智能体（Multi-Agent）架构的、可本地化部署的金融AI服务解决方案。它旨在将强大的硬件资源（如多核CPU服务器）利用最大化，通过容器化技术（Docker）构建一个安全、可扩展、专业的AI知识库与服务平台。

本方案的设计哲学源于一个核心理念：**大语言模型（LLM）作为交互中心，专业的子智能体（Agent）处理特定领域的任务**。这避免了单一全能模型的知识混乱和性能瓶颈，确保了在金融等专业领域中的准确性和可靠性。

## 核心架构

我们采用“分诊台 + 专员”的模式，将不同的用户请求路由到最合适的智能体进行处理。

```mermaid
graph TD
    subgraph "用户交互层"
        User[用户]
    end

    subgraph "Docker 容器化环境"
        User -- HTTP --> Router[分诊台 Agent (FastAPI on Port 8000)]

        Router -- 路由决策 --> Marketing[营销专员]
        Router -- 路由决策 --> Complaint[投诉专员]

        Marketing -- 知识检索 --> VectorDB[(Milvus 向量数据库)]
        Complaint -- 读/写 --> SQL_DB[(PostgreSQL 关系型数据库)]

        Marketing -- 构造Prompt --> LLM[Ollama (本地大模型)]
        Complaint -- 构造Prompt --> LLM
        LLM -- 生成回答 --> Router
        Router -- 返回最终答案 --> User
    end

    subgraph "数据存储 (本地持久化卷)"
        VectorDB -- 存储 --> MilvusData[milvus_data]
        SQL_DB -- 存储 --> PostgresData[postgres_data]
        LLM -- 存储 --> OllamaData[ollama_data]
    end
```

*   **分诊台 (Router Agent)**: 作为一个轻量级的API入口，负责理解用户初步意图，并将请求分发给下游的专业智能体。
*   **营销专员 (Marketing Agent)**: 专门处理产品知识、市场信息等查询。它通过**检索增强生成（RAG）**技术，从**向量数据库（Milvus）**中检索相关的知识文档来生成答案。
*   **投诉专员 (Complaint Agent)**: 专门处理用户投诉、操作失败等问题。它被授予了访问**关系型数据库（PostgreSQL）**的权限，可以查询用户行为日志并记录新的投诉信息。
*   **Ollama**: 提供本地化、私有化的大模型推理服务，确保所有数据处理都在内部完成，保障数据安全。

## 技术栈

*   **容器化**: Docker & Docker Compose
*   **LLM服务**: Ollama
*   **后端API**: FastAPI (Python)
*   **向量数据库**: Milvus
*   **关系型数据库**: PostgreSQL
*   **文本嵌入模型**: `BAAI/bge-large-zh-v1.5`
*   **大语言模型**: `qwen:72b` (可替换为其他模型)

## 项目结构

```
fin-agent-suite/
├── backend/
│   ├── scripts/
│   │   └── ingest_data.py   # 数据初始化与入库脚本
│   ├── main.py              # FastAPI 主应用 (包含所有Agent逻辑)
│   ├── Dockerfile           # 后端服务的容器描述文件
│   └── requirements.txt     # Python 依赖
├── knowledge_base/
│   ├── marketing_info.md    # 营销知识库文档
│   └── product_faq.md       # 产品FAQ文档
├── docker-compose.yml       # 容器编排文件
├── .env.example             # 环境变量模板
├── LICENSE                  # MIT 许可证
└── README.md                # 本文档
```

## 部署指南 (Step-by-Step)

### 1. 环境准备

*   安装 [Git](https://git-scm.com/downloads)
*   安装 [Docker Desktop](https://www.docker.com/products/docker-desktop/) (Windows/Mac) 或 Docker Engine (Linux)

### 2. 克隆并配置项目

```bash
# 克隆项目到本地
git clone https://github.com/your-username/fin-agent-suite.git
cd fin-agent-suite

# 从模板创建环境变量文件
cp .env.example .env
```
**无需修改 `.env` 文件**，除非您想更改数据库的默认用户名和密码。

### 3. 填充知识库

将您自己的Markdown格式的资料库文件（`.md`）放入 `knowledge_base/` 文件夹中。我们已预置了两个示例文件。

### 4. 一键启动所有服务

在项目根目录下，执行以下命令：

```bash
docker-compose up --build -d
```

此命令会完成以下工作：
1.  拉取 Ollama, Milvus, PostgreSQL 的官方镜像。
2.  根据 `backend/Dockerfile` 构建您的后端API服务镜像。
3.  在后台启动所有服务。

首次启动会下载大量镜像，请耐心等待。

### 5. 拉取大语言模型

打开一个新的终端，执行以下命令进入Ollama容器并拉取模型。

```bash
# 进入Ollama容器的交互式终端
docker exec -it ollama bash

# 在容器内拉取模型 (以qwen:72b为例，这是一个大模型，会很耗时)
ollama pull qwen:72b

# 拉取完成后，输入 exit 退出容器
exit
```

### 6. 初始化数据

当所有服务都启动后，运行数据初始化脚本。该脚本会：
1.  读取 `knowledge_base/` 中的文档，将其向量化后存入 Milvus。
2.  在 PostgreSQL 中创建 `user_behavior_log` 和 `user_complaints` 表，并插入一些模拟数据。

```bash
docker-compose run --rm backend python scripts/ingest_data.py
```

您将在终端看到数据处理和入库的日志。

### 7. 测试您的AI智能体！

恭喜！您的金融多智能体AI已部署完成，并正在 `http://localhost:8000` 上监听。

#### 测试1：营销专员 (知识库问答)

```bash
curl -X POST http://localhost:8000/chat \
-H "Content-Type: application/json" \
-d '{"query": "请介绍一下什么是竞价盘？"}'
```
**预期输出**: AI会根据`marketing_info.md`的内容，详细解释竞价盘的定义和特点。

#### 测试2：投诉专员 (数据库交互)

```bash
curl -X POST http://localhost:8000/chat \
-H "Content-Type: application/json" \
-d '{"query": "我5月4号登录失败了，非常不满！"}'```
**预期输出**: AI会回复一段安抚性的话语，并明确提到它查询到了“5月4日登录失败”的记录，同时告知投诉已被记录。

## 项目管理

*   **查看所有服务日志**:
    ```bash
    docker-compose logs -f
    ```
*   **查看特定服务日志** (例如后端API):
    ```bash
    docker-compose logs -f backend
    ```
*   **停止并移除所有容器**:
    ```bash
    docker-compose down
    ```
*   **停止并移除容器及所有数据卷** (将清除所有数据库数据，慎用！):
    ```bash
    docker-compose down -v
    ```

## 如何扩展？

### 添加新的智能体

1.  在 `backend/main.py` 中定义一个新的处理函数，例如 `handle_risk_assessment_query()`。
2.  在 `/chat` 路由的逻辑中，添加新的关键词或判断条件，将特定请求路由到您的新函数。

### 更新知识库

1.  在 `knowledge_base/` 文件夹中添加、修改或删除 `.md` 文件。
2.  重新运行**步骤6**中的数据初始化脚本 `ingest_data.py` 来更新向量数据库。

## 贡献

欢迎提交 Pull Requests 或开 Issues 来改进这个项目！

## 许可证

本项目采用 [MIT License](LICENSE)。

---

至此，整个解决方案，包括详细的文档、完整的配置和可运行的代码，现已全部提供。您可以放心地将其分享到GitHub，任何具备基础Docker知识的用户都能根据此指南成功部署和运行。